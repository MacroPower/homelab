# https://github.com/liqotech/liqo/blob/master/deployments/liqo/values.yaml

apiServer:
  # -- The address that must be used to contact your API server, it needs to be reachable from the clusters that you will peer with (defaults to your master IP).
  address: https://kube.home.macro.network:6443
  # -- Indicates that the API Server is exposing a certificate issued by a trusted Certification Authority.
  trustedCA: false

networking:
  # -- Use the default Liqo network manager.
  internal: true
  # -- Reflect pod IPs and EnpointSlices to the remote clusters.
  reflectIPs: true
  # -- Iptables configuration tuning.
  iptables:
    # -- Select the iptables mode to use. Possible values are "legacy" and "nf_tables".
    mode: "nf_tables"
  # -- Set the MTU for the interfaces managed by liqo: vxlan, tunnel and veth interfaces.
  # The value is used by the gateway and route operators.
  mtu: 1420
  # -- Select the mode to enforce security on connectivity among clusters. Possible values are "FullPodToPod" and "IntraClusterTrafficSegregation"
  securityMode: "IntraClusterTrafficSegregation"

reflection:
  skip:
    # -- List of labels that must not be reflected on remote clusters.
    labels: []
    # -- List of annotations that must not be reflected on remote clusters.
    annotations:
      - cloud.google.com/neg
      - cloud.google.com/neg-status
      - kubernetes.digitalocean.com/load-balancer-id
      - ingress.kubernetes.io/backends
      - ingress.kubernetes.io/forwarding-rule
      - ingress.kubernetes.io/target-proxy
      - ingress.kubernetes.io/url-map
      - metallb.universe.tf/address-pool
      - metallb.universe.tf/ip-allocated-from-pool
      - metallb.universe.tf/loadBalancerIPs
      - loadbalancer.openstack.org/load-balancer-id
  pod:
    # -- The number of workers used for the pods reflector. Set 0 to disable the reflection of pods.
    workers: 10
  service:
    # -- The number of workers used for the services reflector. Set 0 to disable the reflection of services.
    workers: 3
    # -- The type of reflection used for the services reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
    # -- List of load balancer classes that will be shown to remote clusters. If empty, load balancer classes will be reflected as-is.
    # Example:
    # loadBalancerClasses:
    # - name: public
    #   default: true
    # - name: internal
    loadBalancerClasses: []
  endpointslice:
    # -- The number of workers used for the endpointslices reflector. Set 0 to disable the reflection of endpointslices.
    workers: 10
  ingress:
    # -- The number of workers used for the ingresses reflector. Set 0 to disable the reflection of ingresses.
    workers: 3
    # -- The type of reflection used for the ingresses reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
    # -- List of ingress classes that will be shown to remote clusters. If empty, ingress class will be reflected as-is.
    # Example:
    # ingressClasses:
    # - name: nginx
    #   default: true
    # - name: traefik
    ingressClasses: []
  configmap:
    # -- The number of workers used for the configmaps reflector. Set 0 to disable the reflection of configmaps.
    workers: 3
    # -- The type of reflection used for the configmaps reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
  secret:
    # -- The number of workers used for the secrets reflector. Set 0 to disable the reflection of secrets.
    workers: 3
    # -- The type of reflection used for the secrets reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList
  serviceaccount:
    # -- The number of workers used for the serviceaccounts reflector. Set 0 to disable the reflection of serviceaccounts.
    workers: 3
  persistentvolumeclaim:
    # -- The number of workers used for the persistentvolumeclaims reflector. Set 0 to disable the reflection of persistentvolumeclaims.
    workers: 3
  event:
    # -- The number of workers used for the events reflector. Set 0 to disable the reflection of events.
    workers: 3
    # -- The type of reflection used for the events reflector. Ammitted values: "DenyList", "AllowList".
    type: DenyList

controllerManager:
  # -- The number of controller-manager instances to run, which can be increased for active/passive high availability.
  replicas: 2
  pod:
    # -- Resource requests and limits (https://kubernetes.io/docs/user-guide/compute-resources/) for the controller-manager pod.
    resources:
      limits: {}
      requests: {}
  config:
    # -- Percentage of available cluster resources that you are willing to share with foreign clusters.
    resourceSharingPercentage: 80
    # -- Threshold (in percentage) of the variation of resources that triggers a ResourceOffer update. E.g., when the available resources grow/decrease by X, a new ResourceOffer is generated.
    offerUpdateThresholdPercentage: ""
    # -- The address of an external resource plugin service (see https://github.com/liqotech/liqo-resource-plugins for additional information), overriding the default resource computation logic based on the percentage of available resources. Leave it empty to use the standard local resource monitor.
    resourcePluginAddress: ""
    # -- It enforces offerer-side that offloaded pods do not exceed offered resources (based on container limits).
    # This feature is suggested to be enabled when consumer-side enforcement is not sufficient.
    # It has the same tradeoffs of resource quotas (i.e, it requires all offloaded pods to have resource limits set).
    enableResourceEnforcement: false
    # -- Ensure offloaded pods running on a failed node are evicted and rescheduled on a healthy node, preventing them to remain in a terminating state indefinitely.
    # This feature can be useful in case of remote node failure to guarantee better service continuity and to have the expected pods workload on the remote cluster.
    # However, enabling this feature could produce zombies in the worker node, in case the node returns Ready again without a restart.
    enableNodeFailureController: false
  metrics:
    serviceMonitor:
      enabled: true

route:
  pod:
    resources:
      limits: {}
      requests: {}

gateway:
  # -- The number of gateway instances to run.
  # The gateway component supports active/passive high availability.
  # Make sure that there are enough nodes to accommodate the replicas, because such pod has to run in the host network, hence
  # no more than one replica can be scheduled on a given node.
  replicas: 2
  pod:
    resources:
      limits: {}
      requests: {}
  service:
    type: "LoadBalancer"
    annotations:
      io.cilium/lb-ipam-ips: "10.10.30.3"
    labels:
      bgp.kubernetes.macro.network/peer_group: cbgp
  config:
    # Note: This is added to the *external* DNS server.
    addressOverride: liqo-gateway.home.macro.network
    listeningPort: 5871
  metrics:
    enabled: true
    serviceMonitor:
      enabled: true

networkManager:
  pod:
    resources:
      limits: {}
      requests: {}
  config:
    # -- The subnet used by the pods in your cluster, in CIDR notation (e.g., 10.0.0.0/16).
    podCIDR: "10.128.0.0/14"
    # -- The subnet used by the services in you cluster, in CIDR notation (e.g., 172.16.0.0/16).
    serviceCIDR: "10.112.0.0/12"
    # -- List of IP subnets that do not have to be used by Liqo.
    # Liqo can perform automatic IP address remapping when a remote cluster is peering with you, e.g., in case IP address spaces (e.g., PodCIDR) overlaps.
    # In order to prevent IP conflicting between locally used private subnets in your infrastructure and private subnets belonging to remote clusters
    # you need tell liqo the subnets used in your cluster. E.g if your cluster nodes belong to the 192.168.2.0/24 subnet, then
    # you should add that subnet to the reservedSubnets. PodCIDR and serviceCIDR used in the local cluster are automatically added to the reserved list.
    reservedSubnets:
      - 192.168.0.0/16
      - 172.16.0.0/12
      - 10.0.0.0/10
      - 10.64.0.0/11
      - 10.96.0.0/12
      - 10.132.0.0/14
      - 10.136.0.0/14
      - 10.140.0.0/14
      - 10.144.0.0/12
      - 10.160.0.0/12
      - 10.176.0.0/12
      - 10.192.0.0/12

crdReplicator:
  pod:
    resources:
      limits: {}
      requests: {}
  metrics:
    podMonitor:
      enabled: true

discovery:
  pod:
    resources:
      limits: {}
      requests: {}
  config:
    # -- Specify an unique ID (must be a valid uuidv4) for your cluster, instead of letting helm generate it automatically at install time.
    # You can generate it using the command: `uuidgen`
    # This field is needed when using tools such as ArgoCD, since the helm lookup function is not supported and a new value would be generated at each deployment.
    clusterIDOverride: "57fcb13c-e14d-4564-9cdf-1afc26da8035"
    # -- Set a mnemonic name for your cluster.
    clusterName: "home"
    # -- A set of labels that characterizes the local cluster when exposed remotely as a virtual node.
    # It is suggested to specify the distinguishing characteristics that may be used to decide whether to offload pods on this cluster.
    clusterLabels: {}
     # topology.kubernetes.io/zone: us-east-1
     # liqo.io/provider: your-provider

    # -- Enable the mDNS discovery on LANs, set to false to not look for other clusters available in the same LAN.
    # Usually this feature should be active when you have multiple (tiny) clusters on the same LAN (e.g., multiple K3s running on individual devices);
    # if your clusters operate on the big Internet, this feature is not needed and it can be turned off.
    enableDiscovery: false
    # -- Enable the mDNS advertisement on LANs, set to false to not be discoverable from other clusters in the same LAN.
    # When this flag is 'false', the cluster can still receive the advertising from other (local) clusters, and automatically peer with them.
    enableAdvertisement: false
    # -- Time-to-live before an automatically discovered clusters is deleted from the list of available ones if no longer announced (in seconds).
    ttl: 90
    # -- Automatically join discovered clusters.
    autojoin: true
    # -- Allow (by default) the remote clusters to establish a peering with our cluster.
    incomingPeeringEnabled: true

auth:
  pod:
    labels:
      o11y.jacobcolvin.com/instrument: "beyla"
    resources:
      limits: {}
      requests: {}

  service:
    # -- Kubernetes service used to expose the Authentication Service.
    # If you are exposing this service with an Ingress, you can change it to ClusterIP;
    # if your cluster does not support LoadBalancer services, consider to switch it to NodePort.
    # See https://doc.liqo.io/installation/ for more details.
    type: "ClusterIP"

  # -- Enable TLS for the Authentication Service Pod (using a self-signed certificate).
  # If you are exposing this service with an Ingress, consider to disable it or add the appropriate annotations to the Ingress resource.
  tls: true
  ingress:
    enable: false
  config:
    addressOverride: liqo-auth.home.macro.network
    portOverride: "443"

metricAgent:
  # -- Enable/Disable the virtual kubelet metric agent. This component aggregates all the kubelet-related metrics
  # (e.g., CPU, RAM, etc) collected on the nodes that are used by a remote cluster peered with you, then exporting
  # the resulting values as a property of the virtual kubelet running on the remote cluster.
  enable: true
  config:
    # -- Set the timeout for the metrics server.
    timeout:
      read: 30s
      write: 30s
  pod:
    resources:
      limits: {}
      requests: {}

telemetry:
  # Telemetry is on hooks which is pretty annoying.
  enable: false

virtualKubelet:
  extra:
    resources:
      limits: {}
      requests: {}
  virtualNode:
    extra:
      # -- Extra annotations for the virtual node.
      annotations: {}
      # -- Extra labels for the virtual node.
      labels: {}
  metrics:
    enabled: true
    podMonitor:
      enabled: true

uninstaller:
  pod:
    resources:
      limits: {}
      requests: {}

proxy:
  pod:
    resources:
      limits: {}
      requests: {}

storage:
  # -- Enable/Disable the liqo virtual storage class on the local cluster. You will be able to
  # offload your persistent volumes, while other clusters will be able to schedule their
  # persistent workloads on the current cluster.
  enable: true
  # -- Name to assign to the liqo virtual storage class.
  virtualStorageClassName: liqo
  # -- Name of the real storage class to use in the local cluster.
  realStorageClassName: ""

offloading:
  runtimeClass:
    enable: true
    name: liqo
    nodeSelector:
      enable: true
      labels:
        liqo.io/type: virtual-node
    tolerations:
      enable: true
      tolerations:
        - key: virtual-node.liqo.io/not-allowed
          operator: Exists
          effect: NoExecute
